# ğŸ§  Local Prompt Evaluator

A **full-stack, locally-running AI framework** that evaluates the quality of prompt-response pairs using a fine-tuned **DistilBERT** model. No paid API calls needed for evaluation â€” everything runs on your machine.

---

## ğŸ¯ What It Does

You provide:
- **A Prompt** â€” the question or instruction given to an AI
- **An AI Response** â€” the answer produced by any AI model

The system returns:
- **Score** â€” quality rating from `0.0` to `5.0`
- **Quality Label** â€” `poor` / `fair` / `good` / `excellent`
- **Confidence** â€” how certain the model is (0â€“100%)

---

## âœ¨ Features

- âœ… **Local inference** â€” no API calls, fully offline evaluation
- âœ… **Fine-tuned DistilBERT** â€” trained on the `nvidia/HelpSteer2` dataset
- âœ… **REST API** â€” FastAPI backend for easy integration
- âœ… **React Frontend** â€” clean UI to evaluate prompts interactively
- âœ… **Batch Evaluation** â€” evaluate multiple prompt-response pairs at once
- âœ… **Response Comparison** â€” compare multiple AI responses to the same prompt and rank them

---

## ğŸ—ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **ML Model** | DistilBERT (HuggingFace Transformers) |
| **Training Data** | nvidia/HelpSteer2 |
| **Backend** | FastAPI + Uvicorn |
| **Frontend** | React.js + Vite |
| **Deep Learning** | PyTorch |
| **Language** | Python 3.10+ / Node.js |

---

## ğŸ“ Project Structure

```
local-prompt-evaluator/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py              # FastAPI REST API
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluator.py        # Core inference engine
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py            # Training loop
â”‚   â”œâ”€â”€ model.py            # DistilBERT model definition
â”‚   â”œâ”€â”€ dataset.py          # PyTorch dataset loader
â”‚   â””â”€â”€ prepare_data.py     # Data preparation script
â”œâ”€â”€ frontend/               # React.js Vite frontend
â”‚   â””â”€â”€ src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ prompt_evaluator/   # Saved fine-tuned model
â”‚   â””â”€â”€ tokenizer/          # Saved tokenizer
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv           # Training data
â”‚   â””â”€â”€ test.csv            # Test data
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_history.csv
â”œâ”€â”€ config.py               # Central configuration
â”œâ”€â”€ main.py                 # CLI entry point
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Installation

### Prerequisites
- Python 3.10+
- Node.js 18+
- Git

### 1. Clone the Repository
```bash
git clone <your-repo-url>
cd local-prompt-evaluator
```

### 2. Set Up Python Environment
```bash
python -m venv .venv
.venv\Scripts\activate        # Windows
# source .venv/bin/activate   # Mac/Linux

pip install -r requirements.txt
```

### 3. Install Frontend Dependencies
```bash
cd frontend
npm install
cd ..
```

---

## ğŸš€ Quick Start

### Start the Backend (Terminal 1)
```bash
python -m uvicorn api.app:app --host 0.0.0.0 --port 8000 --reload
```
Backend will be available at: **http://localhost:8000**

### Start the Frontend (Terminal 2)
```bash
cd frontend
npm run dev
```
Frontend will be available at: **http://localhost:3000**

---

## ğŸ–¥ï¸ Usage

### Option 1: Frontend UI
1. Open **http://localhost:3000** in your browser
2. Enter your **Prompt** in the first text box
3. Enter the **AI Response** in the second text box
4. Click **Evaluate**
5. View your score, quality label, and confidence

### Option 2: REST API

**Evaluate a single prompt-response pair:**
```bash
curl -X POST http://localhost:8000/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is machine learning?",
    "response": "Machine learning is a subset of AI that enables systems to learn from data."
  }'
```

**Response:**
```json
{
  "score": 4.2,
  "quality": "good",
  "confidence": 0.84,
  "binary_score": 0.84,
  "timestamp": "2026-02-23T11:00:00"
}
```

### Option 3: Python Script
```python
from evaluation.evaluator import PromptEvaluator

evaluator = PromptEvaluator()

result = evaluator.evaluate(
    prompt="Explain neural networks.",
    response="Neural networks are computing systems inspired by the human brain."
)

print(f"Score: {result['score']:.2f}/5.0")
print(f"Quality: {result['quality']}")
print(f"Confidence: {result['confidence']*100:.1f}%")
```

---

## ğŸ“Š Quality Scoring

| Label | Score Range | Binary Score |
|---|---|---|
| â­ Excellent | 4.0 â€“ 5.0 | â‰¥ 0.8 |
| ğŸŸ¢ Good | 3.0 â€“ 3.99 | â‰¥ 0.6 |
| ğŸŸ¡ Fair | 2.0 â€“ 2.99 | â‰¥ 0.4 |
| ğŸ”´ Poor | 0.0 â€“ 1.99 | < 0.4 |

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/` | API health check |
| `GET` | `/health` | Detailed health status |
| `POST` | `/evaluate` | Evaluate a prompt-response pair |
| `POST` | `/evaluate/batch` | Evaluate multiple pairs |
| `POST` | `/compare` | Compare multiple responses to one prompt |
| `GET` | `/model/info` | Get model metadata |

---

## ğŸ‹ï¸ Training

### Prepare Data
```bash
python main.py prepare
```

### Train the Model
```bash
python main.py train
```

### Configuration (`config.py`)
| Parameter | Default | Description |
|---|---|---|
| `epochs` | `1` | Number of training epochs |
| `batch_size` | `8` | Training batch size |
| `learning_rate` | `2e-5` | AdamW learning rate |
| `max_samples` | `500` | Dataset sample limit |
| `max_length` | `512` | Token sequence length |

### Training Results
| Metric | Train | Test |
|---|---|---|
| Accuracy | 80.4% | 82.6% |
| F1 Score | 0.890 | 0.905 |
| Loss | 0.552 | 0.456 |

---

## ğŸ”§ Configuration

All settings are managed in `config.py`. You can also override via environment variables:

```bash
EPOCHS=5
BATCH_SIZE=16
LEARNING_RATE=2e-5
DEVICE=cuda         # Use GPU if available
API_PORT=8000
```

---

## ğŸ“¦ Dependencies

```
torch
transformers
datasets
pandas
scikit-learn
tqdm
fastapi
uvicorn
pydantic
```

---

## ğŸ“ˆ Model Architecture

- **Base Model:** `distilbert-base-uncased` (66M parameters)
- **Task:** Binary classification â†’ scaled to 0â€“5 score
- **Input:** `Prompt: {prompt}\n\nResponse: {response}` (max 512 tokens)
- **Output:** Probability score â†’ quality label

---

## ğŸ—’ï¸ License

This project is for educational and research purposes.

---

## ğŸ‘¨â€ğŸ’» Author

Built as a full-stack prompt evaluation framework using the **LLM-as-a-judge** pattern with local DistilBERT fine-tuning.
